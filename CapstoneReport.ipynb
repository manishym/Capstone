{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Capstone Project\n",
    "## Machine Learning Engineer Nanodegree\n",
    "\n",
    "### Manish Yathnalli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4eaf1444e4eb9f6d93a207e9ea23db911acaa824"
   },
   "source": [
    "# Definition\n",
    "## Project Overview\n",
    "Stock prediction problem is one of the most interesting applications of machine learning.  Entire finincial technology world is focused on solving the problem of predicting the stock value reliably. My goal originally was to use a LSTM to predict the confidance value of the stock going up or down next ten days. Given that this was a compitition in Kaggle  and the restictions were that we cannot use GPU, the LSTM approach did not work that well, however I did my best.\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "The problem is forcasting stock prices. We get historical stock and news data from 2007 to present. We need to predict a signed confidence value \n",
    "$$\n",
    "\\hat y_{ti} \\in [-1, 1] \n",
    "$$\n",
    "which is multiplied by a market adjusted return of a given asset over a ten day window.\n",
    "If the algorithm thinks that the stock will go up and it is very confident, it will return a value close to 1.\n",
    "\n",
    "## Metrics\n",
    "Given the above predicted $ \\hat y_{ti} $, the return will be calculated as  \n",
    "\n",
    "$$\n",
    "x_t = \\sum_i{\\hat y_{ti}r_{ti}u_{ti}}\n",
    "$$\n",
    "Where,  \n",
    "$r_{ii}$ is the 10 day market adjusted return for day $t$ for the instrument $i$  \n",
    "$u_{ti}$ is a variable that controls whether a particular asset is included in scoring on a particular day.\n",
    "The score for submission, hence the performance of the model is decided by the standard deviation for the daily $x_t$ values\n",
    "$$\n",
    "score = \\frac {\\bar x_t} {\\sigma(x_t)}\n",
    "$$\n",
    "Given that this is a real competition the results will be out after six months and who ever got closest to the future price in the 6 month duration will be the winner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60240ac90a575e4ed9a07b5f26e9b66e36d1ca65"
   },
   "source": [
    "## Data exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the target variable\n",
    "The target variable in this case is  `returnsOpenNextMktres10` This is not exactly the returns of the stock. It is some kind of sharpe ratio, which discounts the market return from the stock return. Lets just figure out what this is by doing some analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f9257bf28fe5241b9034adacf563e830a5843fa"
   },
   "source": [
    "## Algorithms\n",
    "We will use LSTM Network to predict the stock prices. LSTM are special kind of Reccurant Neural Networks what work well on remembering long term dependencies. An RNN contains a single repeating neural network like below.\n",
    "![RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "\n",
    "LSTMs also have this structure, however with 4 neural networks instead of single one, which act in a combined way to learn long term dependencies\n",
    "\n",
    "![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "The top line in the diagram is a cell state. The neural networks act as gates to control the flow of information in the top line. The x symbol indicates a point wise multiplication. The value sigmoid layer outputs is used for the point wise multiplication. If the sigmoid value is small, then very little information from the input flows forward, if the sigmoid is close to 1, then most of the input flows forward, hence the gate analogy.\n",
    "\n",
    "The first gate decides what information must be thrown away from the cell state.\n",
    "![Forget gate layer](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "It looks at hidden outputs of previous predection and the current input and outputs a number between 0 and 1, which will be multipled with the cell state of previous prediction in order to *Forget* some of the previous cell state.\n",
    "\n",
    "Next two layers are sigmod and tanh layers, which are called as Input Gate Layer, which decide which parts of hidden and input should be used as input to the next layer.\n",
    "![Input Gate Layers](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "Finally, we will run a sigmoid on hidden state and scale it by output of sigmoid of input gate layer and pass it through tanh layer and combine it with the cell state that we got as output from applying the forget gate.\n",
    "\n",
    "The reason that this works well in remembering the long term dependencies is that we learn what to forget from previous states and what to use from the new input every time.  \n",
    "This explaination is just to set the context, we will just use LSTM cell provided by tensorflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
