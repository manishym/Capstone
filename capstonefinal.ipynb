{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Capstone Project\n## Machine Learning Engineer Nanodegree\n\n### Manish Yathnalli"},{"metadata":{"_uuid":"4eaf1444e4eb9f6d93a207e9ea23db911acaa824"},"cell_type":"markdown","source":"# Definition\n## Project Overview\nStock prediction problem is one of the most interesting applications of machine learning.  Entire finincial technology world is focused on solving the problem of predicting the stock value reliably. My goal originally was to use a LSTM to predict the confidance value of the stock going up or down next ten days. Given that this was a compitition in Kaggle  and the restictions were that we cannot use GPU, the LSTM approach did not work that well, however I did my best.\n\n\n## Problem Statement\nThe problem is forcasting stock prices. We get historical stock and news data from 2007 to present. We need to predict a signed confidence value \n$$\n\\hat y_{ti} \\in [-1, 1] \n$$\nwhich is multiplied by a market adjusted return of a given asset over a ten day window.\nIf the algorithm thinks that the stock will go up and it is very confident, it will return a value close to 1.\n\n## Metrics\nGiven the above predicted $ \\hat y_{ti} $, the return will be calculated as  \n\n$$\nx_t = \\sum_i{\\hat y_{ti}r_{ti}u_{ti}}\n$$\nWhere,  \n$r_{ii}$ is the 10 day market adjusted return for day $t$ for the instrument $i$  \n$u_{ti}$ is a variable that controls whether a particular asset is included in scoring on a particular day.\nThe score for submission, hence the performance of the model is decided by the standard deviation for the daily $x_t$ values\n$$\nscore = \\frac {\\bar x_t} {\\sigma(x_t)}\n$$\nGiven that this is a real competition the results will be out after six months and who ever got closest to the future price in the 6 month duration will be the winner.\n"},{"metadata":{"_uuid":"60240ac90a575e4ed9a07b5f26e9b66e36d1ca65"},"cell_type":"markdown","source":"## Data exploration\n"},{"metadata":{"trusted":true,"_uuid":"f4560699166a904379cf16f819fb08ddb9d8c653"},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market, news) = env.get_training_data()\n\n# Will use daily data only\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfc27757909cd5239eb32b6a10a26b38de89ff46"},"cell_type":"code","source":"import pandas as pd\nnews.time = pd.to_datetime(news.time.astype('datetime64').dt.date, utc=True)\nmarket.time = pd.to_datetime(market.time.astype('datetime64').dt.date, utc=True)\n\nmarket.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e81acd307b6e30df9b3b97a42bf5c71963763f3"},"cell_type":"code","source":"market.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65276540b78fa8bc600f5238098d67304c360e9d"},"cell_type":"code","source":"market.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c1d61ef58af0ce137e581a645087b5420d9551e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bcbe053d4bbe0ea18498ae9beefc131e20433e4"},"cell_type":"markdown","source":"### What is the target variable\nThe target variable in this case is  `returnsOpenNextMktres10` This is not exactly the returns of the stock. It is some kind of sharpe ratio, which discounts the market return from the stock return. Lets just figure out what this is by doing some analysis."},{"metadata":{"trusted":true,"_uuid":"d52f1dd45743d4e07117a8a1b1c2f3b72b1228ab"},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf = (\n    market.\n    reset_index().\n    sort_values(['assetCode', 'time']).\n    set_index(['assetCode','time'])\n)\n\ndf['implied_mkt_return'] = (\n    df.\n    groupby('assetCode').\n    apply(lambda x: x.returnsClosePrevRaw1 - x.returnsClosePrevMktres1).\n    reset_index(0, drop=True)\n)\nplt.scatter(\n    df.loc['AAPL.O'].implied_mkt_return,\n    df.loc['NFLX.O'].implied_mkt_return,\n    alpha=0.6\n);\nplt.xlabel('Implied Market Return from AAPL');\nplt.ylabel('Implied Market Return from NFLX');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a69b77c5a3d45c607e9942884f4db2b403cade"},"cell_type":"markdown","source":"If the above plot would have been a straight line, then we could have concluded that target is simple discount of market return over stock return. However that is not the case."},{"metadata":{"trusted":true,"_uuid":"1aac4c0f06389ccbfbb1e6028640794339852cbc"},"cell_type":"code","source":"returnsClosePrevRaw1 = (\n    df['returnsClosePrevRaw1'].\n    swaplevel().\n    unstack()\n)\n\nreturnsClosePrevMktres1 = (\n    df['returnsClosePrevMktres1'].\n    swaplevel().\n    unstack()\n)\n\nnum_days = 260*5  # Take 5 years\nnum_stocks = 200  # Try for 200 stocks but we will get many less due to NaNs\n\nreturnsClosePrevRaw1 = \\\n    returnsClosePrevRaw1.iloc[-num_days:, 0:num_stocks].dropna(axis=1)\nnum_stocks = len(returnsClosePrevRaw1.columns)\nprint(num_stocks)\nreturnsClosePrevMktres1 = (\n    returnsClosePrevMktres1.\n    loc[returnsClosePrevRaw1.index][returnsClosePrevRaw1.columns].\n    clip(lower=-0.15, upper=0.15)\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb0ead3cf840718cd9ec848a991119c6ad64e826"},"cell_type":"markdown","source":"### Extracting the hidden market return\nWe will use Principal component analysis to extract the hidden features in returns data by fitting the PCA model to `returnsClosePrevRaw1` and see if we can find what is the target variable."},{"metadata":{"trusted":true,"_uuid":"557c4b8fa2e7e921aa947fe4365440db127d7d33"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=15, svd_solver='full')\npca.fit(returnsClosePrevRaw1)\nplt.bar(range(15),pca.explained_variance_ratio_);\nplt.title('Principal Components Sorted by Variance Explain');\nplt.ylabel('% of Total Variance Explained');\nplt.xlabel('PC factor number');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a20c0b16a9920bd490ccd0c0932d7fc86d137d67"},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\npcs = pca.transform(returnsClosePrevRaw1)\nmkt_return = pcs[:,0].reshape(num_days,1)\n\n# the betas of each stock to the market return are in\n# the first column of the components\nmkt_beta = pca.components_[0,:].reshape(num_stocks,1)\n\n# the market portion of returns is the projection of one onto the other\nmkt_portion = mkt_beta.dot(mkt_return.T).T\n\n# ...and the residual is just the difference\nresidual = returnsClosePrevRaw1 - mkt_portion\nfrom sklearn.covariance import LedoitWolf\n\ndef get_corr_from_cov(covmat):\n    d = np.diag(np.sqrt(np.diag(lw.covariance_)))\n    return np.linalg.inv(d).dot(lw.covariance_).dot(np.linalg.inv(d))\n\nlw = LedoitWolf()\n\nlw.fit(returnsClosePrevMktres1)\ncorr = get_corr_from_cov(lw.covariance_)\n\nlw.fit(residual)\ncorr2 = get_corr_from_cov(lw.covariance_)\nfrom scipy.spatial import distance\nfrom scipy.cluster import hierarchy\n\ndef plot_side_by_side_hm(corr, corr2, title1, title2):\n    row_linkage = hierarchy.linkage(\n        distance.pdist(corr), method='average')\n    row_order = list(map(int, hierarchy.dendrogram(row_linkage, no_plot=True)['ivl']))\n    \n    col_linkage = hierarchy.linkage(\n        distance.pdist(corr.T), method='average')\n    col_order = list(map(int, hierarchy.dendrogram(col_linkage, no_plot=True)['ivl']))\n    \n    corr_swapped = np.copy(corr)\n    corr_swapped[:, :] = corr_swapped[row_order, :]\n    corr_swapped[:, :] = corr_swapped[:, col_order]\n\n    corr_swapped2 = np.copy(corr2)\n    corr_swapped2[:, :] = corr_swapped2[row_order, :]\n    corr_swapped2[:, :] = corr_swapped2[:, col_order]\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2)\n    fig.tight_layout()\n    cs1 = sns.heatmap(corr_swapped, square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax1, cmap='OrRd')\n    cs1.set_title(title1)\n    cs2 = sns.heatmap(corr_swapped2, square=True, xticklabels=False, yticklabels=False, cbar=False, ax=ax2, cmap='OrRd')\n    cs2.set_title(title2);\n\nplot_side_by_side_hm(\n    corr,\n    corr2,\n    'Hierarchical Correlation Matrix: returnsClosePrevMktres1',\n    'Correlation Matrix: Our Residual Est (mapped to <-- hierarchy)'\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"423ae64ff85dde72b113e127863326c3ac1859db"},"cell_type":"code","source":"\nlw.fit(returnsClosePrevRaw1)\ncorr = get_corr_from_cov(lw.covariance_)\n\nlw.fit(returnsClosePrevMktres1)\ncorr2 = get_corr_from_cov(lw.covariance_)\nplot_side_by_side_hm(\n    corr,\n    corr2,\n    'Hierarchical Correlation Matrix: returnsClosePrevRaw1',\n    'Correlation Matrix: returnsClosePrevMktres1 (mapped to <-- hierarchy)'\n)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3feb6d29ebb03d9d70d37d9d1799f04483e07f3c"},"cell_type":"markdown","source":"From above graphs we can conclude that our target variable is substraction of per stock beta adjusted market return from the raw return."},{"metadata":{"_uuid":"1d3838bd81a131f5967794a26c9d81c8ea40231a"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"bbe7d49fb6bc30f10fcecf1e82f61f1f933d4ac3"},"cell_type":"code","source":"market.returnsOpenNextMktres10.plot(figsize=(12,5))\nplt.title('Label values: returnsOpenNextMktres10')\nplt.ylabel('returnsOpenNextMktres10')\nplt.xlabel('Observation no')\nplt.show()\n\n# Look at quantiles\nmarket.returnsOpenNextMktres10.describe(percentiles=[0.01, 0.99])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f616f0852f80ea141f560f142c4bbbf4a139907"},"cell_type":"markdown","source":"From above graph and description, we can see that most of the data lie between -0.2 to 0.2, however, there are some outliers that are much higher"},{"metadata":{"trusted":true,"_uuid":"03cbe6ff4f2cb517125eef7afce02e3dc1bcaea6"},"cell_type":"code","source":"sns.distplot(market.returnsOpenNextMktres10.clip(-1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c480f949e4725830478addcc745b2a8791f7a98a"},"cell_type":"markdown","source":"### News Data EDA\n"},{"metadata":{"trusted":true,"_uuid":"28560456c63203b44e475b5e77de356f8fb0075e"},"cell_type":"code","source":"news.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346ea8804c645aadcf416c864939c47a69507dcf"},"cell_type":"code","source":"news.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ac5e41267cbbb0c5a5d304911c96ed529cac76"},"cell_type":"markdown","source":"#### News Sentiment"},{"metadata":{"trusted":true,"_uuid":"3edceca6d8d9dcc6681dc95bc390ad39811ca122"},"cell_type":"code","source":"    # Barplot on negative, neutral and positive columns.\n    news[['sentimentNegative', 'sentimentNeutral','sentimentPositive']].mean().plot(kind='bar')\n    plt.title(\"News positivity chart\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ef3061256d2ca7f391e70a82a03c1598201514"},"cell_type":"markdown","source":"## Data Preprocessing\n### Preprocess Market and News Data"},{"metadata":{"trusted":true,"_uuid":"2bfc2bb6dfe971914100d85f4ad54720d834b912"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef train_test_val_split(market):\n    \"\"\"\n    Get sample of assets but each asset has full market data after 2009\n    Split to time sorted train, validation and test.\n    @return: train, validation, test df. Short variant - time and asset columns only\n    \"\"\"\n    # Work with data after 2009\n    market_idx = market[market.time > '2009'][['time', 'assetCode']]\n    market_idx = market_idx.sample(1000000)\n    # Split to train, validation and test\n    market_idx = market_idx.sort_values(by=['time'])\n    market_train_idx, market_test_idx = train_test_split(market_idx, shuffle=False, random_state=24)\n    market_train_idx, market_val_idx = train_test_split(market_train_idx, test_size=0.1, shuffle=False, random_state=24)\n    return(market_train_idx, market_val_idx, market_test_idx)\n\n# Split\ntrain_idx, val_idx, test_idx = train_test_val_split(market)\n\n# Plot train/val/test size\nsns.barplot(['Train', 'Validation', 'Test'],[train_idx.index.size, val_idx.index.size, test_idx.index.size])\nplt.title('Train, validation, test split.')\nplt.ylabel('Count')\nplt.show()\n\ntrain_idx.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c72ea20cb39653f87bc10cf4336641c48b2e389"},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nclass MarketPrepro:\n    \"\"\"\n    Standard way to generate batches for model.fit_generator(generator, ...)\n    Should be fit on train data and used on all train, validation, test\n    \"\"\"\n    # Features\n    assetcode_encoded = []\n    time_cols = ['year', 'week', 'day', 'dayofweek']\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                    'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n                    'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    feature_cols = ['assetCode_encoded'] + time_cols + numeric_cols\n\n    # Labels\n    label_cols = ['returnsOpenNextMktres10']\n\n    def __init__(self):\n        self.cats = {}\n        self.numeric_scaler = StandardScaler()\n\n    def fit(self, market_train_idx, market):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        Store given indices to generate batches_from.\n        @param market_train_df: train data to fit on\n        \"\"\"\n        market_train_df = market.loc[market_train_idx.index].copy()\n        # Clean bad data. We fit on train dataset and it's ok to remove bad data\n        market_train_df = self.fix_train(market_train_df)\n\n        # Extract day, week, year from time\n        market_train_df = self.prepare_time_cols(market_train_df)\n        # Fit for numeric and time\n        # self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(market_train_df[self.numeric_cols + self.time_cols])\n\n        # Fit asset encoding\n        self.encode_asset(market_train_df, is_train=True)\n\n    def fix_train(self, train_df):\n        \"\"\"\n        Remove bad data. For train dataset only\n        \"\"\"\n        # Remove strange cases with close/open ratio > 2\n        max_ratio = 2\n        train_df = train_df[(train_df['close'] / train_df['open']).abs() <= max_ratio].loc[:]\n        # Fix outliers etc like for test set\n        train_df = self.safe_fix(train_df)\n        return train_df\n\n    def safe_fix(self, df):\n        \"\"\"\n        Fill na, fix outliers. Safe for test dataset, no rows removed.\n        \"\"\"\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Fix outliers\n        df[self.numeric_cols] = df[self.numeric_cols].clip(df[self.numeric_cols].quantile(0.01),\n                                                           df[self.numeric_cols].quantile(0.99), axis=1)\n        return df\n\n    def get_X(self, df):\n        \"\"\"\n        Preprocess and return X without y\n        \"\"\"\n        df = df.copy()\n        df = self.safe_fix(df)\n\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Encode assetCode\n        df = self.encode_asset(df)\n        # Scale numeric features and labels\n\n        df = df.set_index(['assetCode', 'time'], drop=False)\n        df[self.numeric_cols + self.time_cols] = self.numeric_scaler.transform(\n            df[self.numeric_cols + self.time_cols].astype(float))\n\n        # print(df.head())\n        # Return X\n        return df[self.feature_cols]\n\n    def get_y(self, df, is_raw_y=False):\n        if is_raw_y:\n            return df[self.label_cols]\n        else:\n            return (df[self.label_cols] >= 0).astype(float)\n\n    def encode_asset(self, df, is_train=False):\n        def encode(assetcode):\n            \"\"\"\n            Encode categorical features to numbers\n            \"\"\"\n            try:\n                # Transform to index of name in stored names list\n                index_value = self.assetcode_encoded.index(assetcode) + 1\n            except ValueError:\n                # If new value, add it to the list and return new index\n                self.assetcode_encoded.append(assetcode)\n                index_value = len(self.assetcode_encoded)\n\n            # index_value = 1.0/(index_value)\n            index_value = index_value / (self.assetcode_train_count + 1)\n            return (index_value)\n\n        # Store train assetcode_train_count for use as a delimiter for test data encoding\n        if is_train:\n            self.assetcode_train_count = len(df['assetCode'].unique()) + 1\n\n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return (df)\n\n    @staticmethod\n    def prepare_time_cols(df):\n        \"\"\"\n        Extract time parts, they are important for time series\n        \"\"\"\n        df['year'] = pd.to_datetime(df['time']).dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = pd.to_datetime(df['time']).dt.day\n        # Week of year\n        df['week'] = pd.to_datetime(df['time']).dt.week\n        df['dayofweek'] = pd.to_datetime(df['time']).dt.dayofweek\n        return df\n\n    \n# Create instance for global usage\nmarket_prepro = MarketPrepro()\nmarket_prepro.fit(train_idx, market)\nprint('market_prepro is fit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e4c60f6ed3382b5066d756064cca1482adca5a1"},"cell_type":"code","source":"from itertools import chain\nclass NewsPrepro:\n    \"\"\"\n    Aggregate news by day and asset. Normalize numeric values.\n    \"\"\"\n    news_cols_numeric = ['urgency', 'takeSequence', 'wordCount', 'sentenceCount', 'companyCount',\n                         'marketCommentary', 'relevance', 'sentimentNegative', 'sentimentNeutral',\n                         'sentimentPositive', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n                         'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n                         'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n\n    feature_cols = news_cols_numeric\n\n    def fit(self, idx, news):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        @param idx: index with time, assetCode\n        \"\"\"\n        # Save indices[assetCode, time, news_index] for all news\n        self.all_news_idx = self.news_idx(news)\n\n        # Get news only related to market idx\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_train_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            self.news_cols_numeric]\n\n        # Numeric data normalization\n        self.numeric_scaler = StandardScaler()\n        news_train_df.fillna(0, inplace=True)\n\n        # Fit scaler\n        self.numeric_scaler.fit(news_train_df)\n\n    def get_X(self, idx, news):\n        \"\"\"\n        Preprocess news for asset code and time from given index\n        \"\"\"\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            ['time', 'assetCode'] + self.news_cols_numeric]\n        news_df = self.aggregate_news(news_df)\n\n        return self.safe_fix(news_df)\n\n    def safe_fix(self, news_df):\n        \"\"\"\n        Scale, fillna\n        \"\"\"\n        # Normalize, fillna etc without removing rows.\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df[self.news_cols_numeric] = self.numeric_scaler.transform(news_df[self.news_cols_numeric])\n        return news_df\n\n    def news_idx(self, news):\n        \"\"\"\n        Get asset code, time -> news id\n        :param news:\n        :return:\n        \"\"\"\n\n        # Fix asset codes (str -> list)\n        asset_codes_list = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*asset_codes_list))\n\n        assetCodes_index = news.index.repeat(asset_codes_list.apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'news_index': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news[['time']], left_on='news_index', right_index=True)\n        # df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def with_asset_code(self, news):\n        \"\"\"\n        Update news index to be time, assetCode\n        :param news:\n        :return:\n        \"\"\"\n        if news.empty:\n            if 'assetCode' not in news.columns:\n                news.columns = news.columns + 'assetCode'\n            return news\n\n        # Fix asset codes (str -> list)\n        news['assetCodesList'] = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*news['assetCodesList']))\n\n        assetCodes_index = news.index.repeat(news['assetCodesList'].apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def aggregate_news(self, df):\n        \"\"\"\n        News are rare for an asset. We get mean value for 10 days\n        :param df:\n        :return:\n        \"\"\"\n        if df.empty:\n            return df\n\n        # News are rare for the asset, so aggregate them by rolling period say 10 days\n        rolling_days = 10\n        df_aggregated = df.groupby(['assetCode', 'time']).mean().reset_index(['assetCode', 'time'])\n        df_aggregated = df_aggregated.groupby('assetCode') \\\n            .rolling(rolling_days, on='time') \\\n            .apply(np.mean, raw=False) \\\n            .reset_index('assetCode')\n        #df_aggregated.set_index(['time', 'assetCode'], inplace=True)\n        return df_aggregated\n    \n\n# Create instance for global usage\nnews_prepro = NewsPrepro()\nnews_prepro.fit(train_idx, news)\nprint('news_prepro is fit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eafaa312052a399b66d38e5943689b58a6bc8079"},"cell_type":"code","source":"class JoinedPreprocessor:\n    \"\"\"\n    Join market with news and preprocess\n    \"\"\"\n\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n\n    def get_X(self, market, news):\n        \"\"\"\n        Returns preprocessed market + news\n        :return: X\n        \"\"\"\n        # Market row\n        market_X = self.market_prepro.get_X(market)\n        # One row in news contains many asset codes. Extend it to news_X with one asset code - one row\n        news_idx = self.news_prepro.news_idx(news)\n        news_X = self.news_prepro.get_X(news_idx, news)\n        #news_X.time = news_X.time.astype('datetime64')\n        # X = market X + news X\n        X = market_X.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        X = X.fillna(0)\n        X = X[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        return X\n\n    def get_Xy(self, idx, market, news, is_train=False, is_raw_y=False):\n        \"\"\"\n        Returns preprocessed features and labels for given indices\n        \"\"\"\n        # Get market data for index\n        market_df = market.loc[idx.index]\n        # We can remove bad data in train\n        if is_train:\n            market_df = self.market_prepro.fix_train(market_df)\n        market_Xy = self.market_prepro.get_X(market_df)\n        # Get news data for index\n        news_X = self.news_prepro.get_X(idx, news)\n        #news_X.time = pd.to_datetime(news_X.time, utc=True)\n        #news_X.time = news_X.time.astype('datetime64')\n        # Merge and return\n        Xy = market_Xy.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        Xy = Xy.fillna(0)\n        X = Xy[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        y = self.market_prepro.get_y(market_df, is_raw_y)\n\n        return X, y\n\n    def with_look_back(self, X, y, look_back, look_back_step):\n        \"\"\"\n        Add look back window values to prepare dataset for LSTM\n        \"\"\"\n        look_back_fixed = look_back_step * (look_back // look_back_step)\n        # Fill look_back rows before first\n        first_xrow = X.values[0]\n        first_xrow.shape = [1, X.values.shape[1]]\n        first_xrows = np.repeat(first_xrow, look_back_fixed, axis=0)\n        X_values = np.append(first_xrows, X.values, axis=0)\n\n        if y is not None:\n            first_yrow = y.values[0]\n            first_yrow.shape = [1, y.values.shape[1]]\n            first_yrows = np.repeat(first_yrow, look_back_fixed, axis=0)\n            y_values = np.append(first_yrows, y.values, axis=0)\n\n        # for i in range(0, len(X) - look_back + 1):\n        X_processed = []\n        y_processed = []\n        for i in range(look_back_fixed , len(X_values)):\n            # Add lookback to X\n            x_window = X_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            X_processed.append(x_window)\n            # If input is X only, we'll not output y\n            if y is None:\n                continue\n            # Add lookback to y\n            y_window = y_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            y_processed.append(y_window)\n        # Return Xy for train/test or X for prediction\n        if y is not None:\n            #return np.array(X_processed), np.array(y_processed)\n            return np.array(X_processed), y.values\n        else:\n            return np.array(X_processed)\n\n        \nprepro = JoinedPreprocessor(market_prepro, news_prepro)    \nprint('Preprocessor created')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"491a0090756058c96446558acb7acf7181071ed8"},"cell_type":"code","source":"class JoinedGenerator:\n    \"\"\"\n    Keras standard approach to generage batches for model.fit_generator() call.\n    \"\"\"\n\n    def __init__(self, prepro, idx, market, news):\n        \"\"\"\n        @param preprocessor: market and news join preprocessor\n        @param market: full loaded market df\n        @param news: full loaded news df\n        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n        \"\"\"\n        self.market = market\n        self.prepro = prepro\n        self.news = news\n        self.idx = idx\n\n    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n        \"\"\"\n        Generate batch data for LSTM NN\n        Each cycle in a loop we yield a batch for one training step in epoch.\n        \"\"\"\n        while True:\n            # Get market indices of random assets, sorted by assetCode, time.\n            batch_idx = self.get_random_assets_idx(batch_size)\n\n            # Get X, y data for this batch, containing market and news, but without look back yet\n            X, y = self.prepro.get_Xy(batch_idx, self.market, self.news, is_train)\n            # Add look back data to X, y\n            X, y = self.prepro.with_look_back(X, y, look_back, look_back_step)\n            yield X, y\n\n    def get_random_assets_idx(self, batch_size):\n        \"\"\"\n        Get random asset and it's last market data indices.\n        Repeat for next asset until we reach batch_size.\n        \"\"\"\n        asset_codes = self.idx['assetCode'].unique().tolist()\n\n        # Insert first asset\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        #asset = 'ADBE.O'\n        batch_index_df = self.idx[self.idx.assetCode == asset].tail(batch_size)\n\n        return batch_index_df.sort_values(by=['assetCode', 'time'])\n        \n\n# Train data generator instance\njoin_generator = JoinedGenerator(prepro, train_idx, market, news)\n\n# Validation data generator instance\nval_generator = JoinedGenerator(prepro, val_idx, market, news)\nprint('Generators created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f9257bf28fe5241b9034adacf563e830a5843fa"},"cell_type":"markdown","source":"## Algorithms\nWe will use LSTM Network to predict the stock prices. LSTM are special kind of Reccurant Neural Networks what work well on remembering long term dependencies. An RNN contains a single repeating neural network like below.\n![RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n\nLSTMs also have this structure, however with 4 neural networks instead of single one, which act in a combined way to learn long term dependencies\n\n![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n\nThe top line in the diagram is a cell state. The neural networks act as gates to control the flow of information in the top line. The x symbol indicates a point wise multiplication. The value sigmoid layer outputs is used for the point wise multiplication. If the sigmoid value is small, then very little information from the input flows forward, if the sigmoid is close to 1, then most of the input flows forward, hence the gate analogy.\n\nThe first gate decides what information must be thrown away from the cell state.\n![Forget gate layer](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\nIt looks at hidden outputs of previous predection and the current input and outputs a number between 0 and 1, which will be multipled with the cell state of previous prediction in order to *Forget* some of the previous cell state.\n\nNext two layers are sigmod and tanh layers, which are called as Input Gate Layer, which decide which parts of hidden and input should be used as input to the next layer.\n![Input Gate Layers](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n\nFinally, we will run a sigmoid on hidden state and scale it by output of sigmoid of input gate layer and pass it through tanh layer and combine it with the cell state that we got as output from applying the forget gate.\n\nThe reason that this works well in remembering the long term dependencies is that we learn what to forget from previous states and what to use from the new input every time.  \nThis explaination is just to set the context, we will just use LSTM cell provided by tensorflow.\n\n"},{"metadata":{"trusted":true,"_uuid":"114fb4a96cfcb1f4195948d65395aea80fb998fb"},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\nclass ModelFactory:\n    \"\"\"\n    Generate different models. Actually only one of them is used in the kernel,\n    this factory is for experiments when debugging.\n    \"\"\"\n    # LSTM look back window size\n    look_back=90\n    # In windows size look back each look_back_step days\n    look_back_step=10\n\n    def lstm_128(input_size):\n        model = Sequential()\n        # Add an input layer market + news\n        #input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n        # input_shape=(timesteps, input features)\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n        model.add(LSTM(units=64, return_sequences=True ))\n        model.add(LSTM(units=32, return_sequences=False))\n\n        # Add an output layer\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n        return(model)\n\n    def train(model, toy, join_generator, val_generator):\n        weights_file='best_weights.h5'\n\n        # We'll stop training if no improvement after some epochs\n        earlystopper = EarlyStopping(patience=5, verbose=1)\n\n        # Low, avg and high scor training will be saved here\n        # Save the best model during the traning\n        checkpointer = ModelCheckpoint(weights_file\n                                       #,monitor='val_acc'\n                                       ,verbose=1\n                                       ,save_best_only=True\n                                       ,save_weights_only=True)\n\n        #reduce_lr = ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.001)\n        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n\n        # Set fit parameters\n        # Rule of thumb: steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n        #                validation_steps = TotalvalidationSamples / ValidationBatchSize\n        if toy:\n            batch_size=1000\n            validation_batch_size=1000\n            steps_per_epoch=5\n            validation_steps=2\n            epochs=3\n            look_back=10\n            look_back_step=2\n        else:\n            batch_size=1000\n            validation_batch_size=1000\n            steps_per_epoch=20\n            validation_steps=5\n            epochs=20\n            look_back=90\n            look_back_step=10\n\n        print(f'Toy:{toy}, epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\n        print(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\n        # Fit\n        training = model.fit_generator(join_generator.flow_lstm(batch_size=batch_size\n                                                                , is_train=True\n                                                                , look_back=look_back\n                                                                , look_back_step=look_back_step)\n                                       , epochs=epochs\n                                       , validation_data=val_generator.flow_lstm(batch_size=validation_batch_size\n                                                                                 , is_train=False\n                                                                                 , look_back=look_back\n                                                                                 , look_back_step=look_back_step)\n                                       , steps_per_epoch=steps_per_epoch\n                                       , validation_steps=validation_steps\n                                       , callbacks=[earlystopper, checkpointer, reduce_lr])\n        # Load best weights saved\n        model.load_weights(weights_file)\n        return training\n\n\nmodel = ModelFactory.lstm_128(len(market_prepro.feature_cols) + len(news_prepro.feature_cols))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1bac3b1bf0c8070d4b7de0c688a2b65ccaf08b0"},"cell_type":"code","source":"training = ModelFactory.train(model, False, join_generator, val_generator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d217fab9045f03d5a25837c9ac58695d07ddc62e"},"cell_type":"markdown","source":"## Benchmark\nA benchmark model for this case would be a light gbm model on market and news data. Full code is in the references section."},{"metadata":{"_uuid":"7fabd396f6279f7cf667e6815484faad161811e2"},"cell_type":"markdown","source":"## Model Evaluation and Validation\n"},{"metadata":{"trusted":true,"_uuid":"2b322bbd629c76378b794d74983edef771dbc0fd"},"cell_type":"code","source":"class Predictor:\n    \"\"\"\n    Predict for test data or real prediction\n    \"\"\"\n\n    def __init__(self, prepro, market_prepro, news_prepro, model, look_back, look_back_step):\n        self.prepro = prepro\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        self.model = model\n        self.look_back = look_back\n        self.look_back_step = look_back_step\n\n    def predict(self, market, news):\n        \"\"\"\n        Predict from new received market and news data.\n        :return: predicted y\n        \"\"\"\n        X = self.prepro.get_X(market, news)\n        X = self.prepro.with_look_back(X, None, self.look_back, self.look_back_step)\n        y = self.model.predict(X) * 2 - 1\n        return y\n\n    def predict_idx(self, pred_idx, market, news):\n        \"\"\"\n        Predict for test from indices\n        :return:\n            predicted y, ground truth y\n        \"\"\"\n        # Get preprocessed X, y\n        X_test, y_test = self.prepro.get_Xy(pred_idx, market, news, is_train=False, is_raw_y=True)\n        # Add there look back rows for LSTM\n        X_test, y_test = self.prepro.with_look_back(X_test, y_test, look_back=self.look_back,\n                                                    look_back_step=self.look_back_step)\n        y_pred = self.model.predict(X_test) * 2 - 1\n        return y_pred, y_test\n\n    \npredictor = Predictor( prepro, market_prepro, news_prepro, model, ModelFactory.look_back, ModelFactory.look_back_step)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1303bf65a764f3b3e9ca636d4fa8a0a1492d1fe3"},"cell_type":"code","source":"def predict_on_test():\n    # Predict on last test data\n    pred_size=1000\n    pred_idx = test_idx.tail(pred_size + ModelFactory.look_back)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    #market_df = market.loc[pred_idx.index]\n    #y_test = market_df['returnsOpenNextMktres10'].values\n    # Plot\n    ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n    ax1.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax1.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax1.legend([\"Ground truth\",\"Predicted\"])\n    ax1.set_title(\"Both\")\n    ax1.set_xlabel(\"Epoch\")\n    ax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\n    ax2.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax2.set_title(\"Ground truth\")\n    ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\n    ax3.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax3.set_title(\"Predicted\")\n    plt.tight_layout()\n    plt.show()\n\npredict_on_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e157d0ff33b78a0edd6f27b9ca6d19e26ff0dfe"},"cell_type":"code","source":"def get_score():\n    \"\"\"\n    Calculation of actual metric that is used to calculate final score\n    @param r: returnsOpenNextMktres10\n    @param u: universe\n    where rti is the 10-day market-adjusted leading return for day t for instrument i, and uti is a 0/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.    \n    \"\"\"\n    # Get test sample to calculate score on\n    pred_idx = test_idx #.sample(10000, random_state=24)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)    \n    look_back=ModelFactory.look_back\n    market_df = market.loc[pred_idx.index]\n    r=market_df['returnsOpenNextMktres10'].values#.values[look_back:]\n    u=market_df['universe'].values#.values[look_back:]\n    confidence=y_pred\n    # calculation of actual metric that is used to calculate final score\n    r = r.clip(-1,1) # get rid of outliers. Where do they come from??\n    x_t_i = confidence.reshape(r.shape) * r * u\n\n    #print(x_t_i.iloc[0])\n    d = (market_df['time'].dt.day).values #[look_back:]\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score = mean / std\n    return score\n    \nprint(f\"Sigma score: {get_score()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50a03fdc678c8aabd042fb5f7ab78505b7d399ce"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom time import time\ndef calc_acc():\n    # Get X_test, y_test with look back for LSTM\n    pred_idx = test_idx.sample(10000)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    \n    #y_pred = pd.DataFrame(market_prepro.y_scaler.inverse_transform(model.predict(X_test)))\n    print(\"Accuracy: %f\" % accuracy_score(y_test >= 0, y_pred >= 0))\n    #score = get_score(market_df, confidence, market_df.returnsOpenNextMktres10, market_df.universe)\n    print('Predictions size: ', len(y_pred))\n    print('y_test size:', len(y_test))\n     # Show distribution of confidence that will be used as submission\n    plt.hist(y_test, bins='auto', alpha=0.3)\n    plt.hist(y_pred, bins='auto', alpha=0.3, color='darkorange')\n    plt.legend(['Ground truth', 'Predicted'])\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.title(\"predicted confidence\")\n    plt.show()\n\n# Call accuracy calculation and plot    \ncalc_acc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9e7af3c5cf64e5a5c4d6c73bc7f863ac64e1281"},"cell_type":"code","source":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Predict\n    y_pred = predictor.predict(market_obs_df, news_obs_df)\n    confidence_df=pd.DataFrame(y_pred, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence\n    \ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\nlast_year=None\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n    # For logging\n    cur_year = market_obs_df.iloc[0].time.strftime('%Y')\n    if cur_year != last_year:\n        print(f'Predicting {cur_year}...')\n        last_year = cur_year\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])\n    \nprint(f\"Prediction for {len(predicted_days)} days completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3820c4dbcc748a2570900203c282bddc6017e5cb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e484c2d3c303deddc02bab5972a28a6b4b20509"},"cell_type":"markdown","source":"## Conclusion\nGiven that this competition had rigid rules on not using GPU and not using any external data source as well as being kernel only competition, LSTM was probably not the best solution for this competition. With all the modification I could think of for this model, I could only get to 0.65 public leaderboard score. This just puts me in the top 40%. However, this also demonstrates that if this were a real project, LSTM would still be the best choice to predict stock prices from the news and market data."},{"metadata":{"_uuid":"dac915680aac8b709bc6eebb22544757374d608e"},"cell_type":"markdown","source":"## References\n[Exploratory data analysis](http://)  \n[LSTM concepts ](http://colah.github.io/posts/2015-08-Understanding-LSTMs)  \n[LSTM for stock market](https://www.datacamp.com/community/tutorials/lstm-python-stock-market)  \n[Combining news and market data](https://www.kaggle.com/codlife/a-framework-to-use-news-which-can-imp-score-in-lb)  \n[LSTM baseline for this competition](https://www.kaggle.com/ashkaan/lstm-baseline)  \n[Benchmark](https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data)  \n"},{"metadata":{"trusted":true,"_uuid":"3970fe942f233065cffec03f17d348839a2e0deb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6c000769a6882e6e766993b48143d4bbdf19dfc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4b075942337adf0147c2e2e98dabc91031dfc68"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}